<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Course Syllabus for Statistical Inference and Probabilistic Modeling - SMU Masters in Applied AI">
    <title>Syllabus | Statistical Inference and Probabilistic Modeling | SMU Applied AI</title>
    <link rel="stylesheet" href="../../assets/styles.css">
</head>

<body>
    <!-- Navbar -->
    <nav class="navbar">
        <div class="navbar-content">
            <a href="index.html" class="navbar-back">Course Home</a>
            <div class="navbar-subtitle">Course 1: Syllabus</div>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="container">
        <div class="card">
            <span class="badge badge-foundation">Foundation Phase</span>
            <h1>Course Syllabus</h1>
            <p class="text-muted">Statistical Inference and Probabilistic Modeling • 15 Weeks • 3 Credit Hours</p>

            <h2>Course Description</h2>
            <p>This course provides a rigorous foundation in probability theory and statistical inference, essential for
                understanding and developing machine learning algorithms. Students will master the mathematical
                frameworks that underpin modern AI systems, from basic probability axioms to advanced Bayesian methods
                and probabilistic graphical models.</p>
            <p>Through a combination of theoretical study and practical application, students will develop the ability
                to reason about uncertainty, make principled decisions under incomplete information, and build
                statistical models that learn from data.</p>

            <h2>Prerequisites</h2>
            <ul>
                <li>Calculus (single and multivariable)</li>
                <li>Linear algebra fundamentals (vectors, matrices, basic operations)</li>
                <li>Basic programming experience (Python recommended)</li>
            </ul>

            <h2>Learning Outcomes</h2>
            <p>Upon successful completion of this course, students will be able to:</p>
            <ul class="objectives-list">
                <li>Apply probability theory to model uncertainty in real-world AI systems</li>
                <li>Perform statistical inference and hypothesis testing on complex datasets</li>
                <li>Implement Bayesian reasoning for updating beliefs based on evidence</li>
                <li>Construct and analyze probabilistic graphical models</li>
                <li>Evaluate model performance using rigorous statistical metrics</li>
                <li>Connect statistical foundations to machine learning algorithms</li>
                <li>Apply Monte Carlo methods for computational statistics</li>
                <li>Understand information-theoretic concepts relevant to ML</li>
            </ul>

            <h2>Weekly Schedule</h2>
            <table style="width: 100%; border-collapse: collapse; margin: 1em 0;">
                <thead>
                    <tr style="background-color: var(--smu-blue); color: white;">
                        <th style="padding: 10px; text-align: left;">Week</th>
                        <th style="padding: 10px; text-align: left;">Topic</th>
                        <th style="padding: 10px; text-align: left;">Key Concepts</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="border-bottom: 1px solid #e0e0e0;">
                        <td style="padding: 10px;">1</td>
                        <td style="padding: 10px;">Foundations of Probability</td>
                        <td style="padding: 10px;">Sample spaces, events, axioms, counting</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0; background-color: #f9f9f9;">
                        <td style="padding: 10px;">2</td>
                        <td style="padding: 10px;">Random Variables & Distributions</td>
                        <td style="padding: 10px;">PMF, PDF, CDF, discrete and continuous</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0;">
                        <td style="padding: 10px;">3</td>
                        <td style="padding: 10px;">Expectation & Variance</td>
                        <td style="padding: 10px;">Mean, variance, moments, covariance</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0; background-color: #f9f9f9;">
                        <td style="padding: 10px;">4</td>
                        <td style="padding: 10px;">Joint Distributions & Independence</td>
                        <td style="padding: 10px;">Marginal, joint, conditional distributions</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0;">
                        <td style="padding: 10px;">5</td>
                        <td style="padding: 10px;">Conditional Probability & Bayes' Theorem</td>
                        <td style="padding: 10px;">Bayes' rule, priors, posteriors, likelihood</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0; background-color: #f9f9f9;">
                        <td style="padding: 10px;">6</td>
                        <td style="padding: 10px;">Sampling & Central Limit Theorem</td>
                        <td style="padding: 10px;">Sampling distributions, CLT, LLN</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0;">
                        <td style="padding: 10px;">7</td>
                        <td style="padding: 10px;">Point & Interval Estimation</td>
                        <td style="padding: 10px;">Estimators, bias, confidence intervals</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0; background-color: #f9f9f9;">
                        <td style="padding: 10px;">8</td>
                        <td style="padding: 10px;">Hypothesis Testing</td>
                        <td style="padding: 10px;">Null/alternative, p-values, Type I/II errors</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0;">
                        <td style="padding: 10px;">9</td>
                        <td style="padding: 10px;">Maximum Likelihood Estimation</td>
                        <td style="padding: 10px;">Likelihood function, MLE, Fisher information</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0; background-color: #f9f9f9;">
                        <td style="padding: 10px;">10</td>
                        <td style="padding: 10px;">Bayesian Inference</td>
                        <td style="padding: 10px;">Conjugate priors, MAP estimation, credible intervals</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0;">
                        <td style="padding: 10px;">11</td>
                        <td style="padding: 10px;">Markov Chains & Stochastic Processes</td>
                        <td style="padding: 10px;">Transition matrices, stationary distributions</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0; background-color: #f9f9f9;">
                        <td style="padding: 10px;">12</td>
                        <td style="padding: 10px;">Probabilistic Graphical Models</td>
                        <td style="padding: 10px;">Bayesian networks, Markov random fields</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0;">
                        <td style="padding: 10px;">13</td>
                        <td style="padding: 10px;">Monte Carlo Methods</td>
                        <td style="padding: 10px;">MCMC, Metropolis-Hastings, Gibbs sampling</td>
                    </tr>
                    <tr style="border-bottom: 1px solid #e0e0e0; background-color: #f9f9f9;">
                        <td style="padding: 10px;">14</td>
                        <td style="padding: 10px;">Information Theory Basics</td>
                        <td style="padding: 10px;">Entropy, KL divergence, mutual information</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px;">15</td>
                        <td style="padding: 10px;">Capstone Project</td>
                        <td style="padding: 10px;">Statistical modeling project presentations</td>
                    </tr>
                </tbody>
            </table>

            <h2>Assessment Structure</h2>
            <div class="resources-section">
                <h3>Grade Distribution</h3>
                <ul>
                    <li><strong>Weekly Quizzes (15%):</strong> Short knowledge checks after each module</li>
                    <li><strong>Problem Sets (30%):</strong> Five assignments covering theoretical and computational
                        problems</li>
                    <li><strong>Midterm Exam (20%):</strong> Covers weeks 1-7 (probability and estimation)</li>
                    <li><strong>Final Project (25%):</strong> Statistical modeling capstone project</li>
                    <li><strong>Participation (10%):</strong> Discussion contributions and peer engagement</li>
                </ul>
            </div>

            <h3>Grading Scale</h3>
            <p>A: 90-100% | B: 80-89% | C: 70-79% | D: 60-69% | F: Below 60%</p>

            <h2>Required Textbooks</h2>
            <ul>
                <li><strong>Primary:</strong> "Probability and Statistics for Machine Learning" by Kevin Murphy</li>
                <li><strong>Supplementary:</strong> "All of Statistics" by Larry Wasserman</li>
                <li><strong>Reference:</strong> "Pattern Recognition and Machine Learning" by Christopher Bishop
                    (Chapters 1-2)</li>
            </ul>

            <h2>Course Policies</h2>
            <h3>Late Submissions</h3>
            <p>Assignments submitted late will incur a 10% penalty per day, up to a maximum of 3 days. After 3 days,
                assignments will not be accepted without prior approval from the instructor.</p>

            <h3>Academic Integrity</h3>
            <p>Students are expected to adhere to SMU's Honor Code. Collaboration is encouraged for understanding
                concepts, but all submitted work must be your own. Plagiarism or unauthorized collaboration will result
                in disciplinary action.</p>

            <h3>Attendance</h3>
            <p>Regular attendance and participation are essential. Students are expected to complete all readings before
                class and actively engage in discussions.</p>

            <h2>Support Resources</h2>
            <ul>
                <li><strong>Office Hours:</strong> Tuesdays and Thursdays, 2:00-4:00 PM (or by appointment)</li>
                <li><strong>Teaching Assistants:</strong> Available for problem set help during weekly lab sessions</li>
                <li><strong>Math Support Center:</strong> Additional tutoring available through the university</li>
            </ul>
        </div>

        <!-- Navigation -->
        <div class="card mt-2">
            <div class="btn-group">
                <a href="index.html" class="btn btn-secondary">← Back to Course</a>
                <a href="readings.html" class="btn btn-primary">Course Readings →</a>
            </div>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <p>© 2026 SMU Lyle School of Engineering | Masters in Applied Artificial Intelligence</p>
        <p><a href="../../index.html">Program Home</a></p>
    </footer>
</body>

</html>