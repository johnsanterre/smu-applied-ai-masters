<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Recommended readings for Statistical Inference and Probabilistic Modeling - SMU Masters in Applied AI">
    <title>Readings | Statistical Inference and Probabilistic Modeling | SMU Applied AI</title>
    <link rel="stylesheet" href="../../assets/styles.css">
</head>

<body>
    <!-- Navbar -->
    <nav class="navbar">
        <div class="navbar-content">
            <a href="index.html" class="navbar-back">Course Home</a>
            <div class="navbar-subtitle">Course 1: Readings</div>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="container">
        <div class="card">
            <span class="badge badge-foundation">Foundation Phase</span>
            <h1>Course Readings</h1>
            <p>A curated collection of textbooks, papers, and online resources to support your learning throughout the
                course.</p>

            <h2>Primary Textbooks</h2>
            <div class="reading-item">
                <div class="reading-title">Machine Learning: A Probabilistic Perspective</div>
                <div class="reading-author">Kevin P. Murphy</div>
                <div class="reading-source">MIT Press, 2012 | ISBN: 978-0262018029</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">Comprehensive coverage of probabilistic
                    approaches to machine learning. Chapters 2-5 are especially relevant for this course.</p>
            </div>

            <div class="reading-item">
                <div class="reading-title">All of Statistics: A Concise Course in Statistical Inference</div>
                <div class="reading-author">Larry Wasserman</div>
                <div class="reading-source">Springer, 2004 | ISBN: 978-0387402727</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">Excellent bridge between mathematical statistics
                    and machine learning. Clear, concise proofs with practical examples.</p>
            </div>

            <div class="reading-item">
                <div class="reading-title">Pattern Recognition and Machine Learning</div>
                <div class="reading-author">Christopher M. Bishop</div>
                <div class="reading-source">Springer, 2006 | ISBN: 978-0387310732</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">Classic text with strong Bayesian perspective.
                    Chapters 1-2 and 8 are directly relevant to course topics.</p>
            </div>

            <h2>Supplementary Texts</h2>
            <div class="reading-item">
                <div class="reading-title">Introduction to Probability</div>
                <div class="reading-author">Dimitri P. Bertsekas and John N. Tsitsiklis</div>
                <div class="reading-source">Athena Scientific, 2nd Edition, 2008</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">Excellent for building intuition about
                    probability. Great companion for weeks 1-5.</p>
            </div>

            <div class="reading-item">
                <div class="reading-title">Bayesian Data Analysis</div>
                <div class="reading-author">Andrew Gelman, John B. Carlin, et al.</div>
                <div class="reading-source">CRC Press, 3rd Edition, 2013</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">The definitive reference for Bayesian methods.
                    Especially useful for weeks 10 and 13.</p>
            </div>

            <div class="reading-item">
                <div class="reading-title">Probabilistic Graphical Models: Principles and Techniques</div>
                <div class="reading-author">Daphne Koller and Nir Friedman</div>
                <div class="reading-source">MIT Press, 2009</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">Authoritative text on graphical models.
                    Reference for week 12.</p>
            </div>

            <h2>Academic Papers</h2>
            <div class="reading-item">
                <div class="reading-title">A Tutorial on the Cross-Entropy Method</div>
                <div class="reading-author">P.T. de Boer, D.P. Kroese, S. Mannor, R.Y. Rubinstein</div>
                <div class="reading-source">Annals of Operations Research, 2005</div>
            </div>

            <div class="reading-item">
                <div class="reading-title">An Introduction to MCMC for Machine Learning</div>
                <div class="reading-author">Christophe Andrieu, Nando de Freitas, et al.</div>
                <div class="reading-source">Machine Learning, 2003</div>
            </div>

            <div class="reading-item">
                <div class="reading-title">Equation of State Calculations by Fast Computing Machines</div>
                <div class="reading-author">Nicholas Metropolis, Arianna Rosenbluth, et al.</div>
                <div class="reading-source">The Journal of Chemical Physics, 1953</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">The foundational paper introducing the
                    Metropolis algorithm.</p>
            </div>

            <div class="reading-item">
                <div class="reading-title">A Mathematical Theory of Communication</div>
                <div class="reading-author">Claude E. Shannon</div>
                <div class="reading-source">Bell System Technical Journal, 1948</div>
                <p style="margin-top: 0.5em; color: var(--text-muted);">Shannon's landmark paper that founded
                    information theory. Essential reading for week 14.</p>
            </div>

            <h2>Online Resources</h2>
            <div class="resources-section">
                <h3>Free Online Courses</h3>
                <ul>
                    <li><a href="https://www.khanacademy.org/math/statistics-probability" target="_blank">Khan Academy:
                            Statistics and Probability</a> — Excellent for refreshing fundamentals</li>
                    <li><a href="https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/"
                            target="_blank">MIT OCW 18.05: Introduction to Probability and Statistics</a></li>
                    <li><a href="https://www.coursera.org/learn/bayesian-statistics" target="_blank">Coursera: Bayesian
                            Statistics</a> — Duke University</li>
                </ul>
            </div>

            <div class="resources-section">
                <h3>Interactive Tools</h3>
                <ul>
                    <li><a href="https://seeing-theory.brown.edu/" target="_blank">Seeing Theory</a> — Visual
                        introduction to probability and statistics</li>
                    <li><a href="https://distill.pub/" target="_blank">Distill.pub</a> — Interactive ML research
                        articles with excellent visualizations</li>
                    <li><a href="https://setosa.io/ev/" target="_blank">Explained Visually</a> — Visual explanations of
                        key statistical concepts</li>
                </ul>
            </div>

            <div class="resources-section">
                <h3>Documentation & Tutorials</h3>
                <ul>
                    <li><a href="https://scipy-lectures.org/" target="_blank">Scipy Lecture Notes</a> — Python
                        scientific computing</li>
                    <li><a href="https://numpy.org/doc/stable/reference/random/index.html" target="_blank">NumPy Random
                            Sampling</a> — Official documentation</li>
                    <li><a href="https://docs.pymc.io/" target="_blank">PyMC Documentation</a> — Probabilistic
                        programming in Python</li>
                </ul>
            </div>

            <h2>Readings by Week</h2>
            <div class="resources-section">
                <h3>Weeks 1-5: Probability Foundations</h3>
                <ul>
                    <li>Murphy Ch. 2: Probability</li>
                    <li>Wasserman Ch. 1-4: Probability, Random Variables, Expectation</li>
                    <li>Bertsekas & Tsitsiklis Ch. 1-4</li>
                </ul>
            </div>

            <div class="resources-section">
                <h3>Weeks 6-10: Statistical Inference</h3>
                <ul>
                    <li>Wasserman Ch. 5-12: Convergence, Estimation, Hypothesis Testing</li>
                    <li>Murphy Ch. 3: Generative Models for Discrete Data</li>
                    <li>Gelman Ch. 1-5: Bayesian Foundations</li>
                </ul>
            </div>

            <div class="resources-section">
                <h3>Weeks 11-15: Advanced Topics</h3>
                <ul>
                    <li>Murphy Ch. 17: Markov and Hidden Markov Models</li>
                    <li>Koller & Friedman Ch. 1-3: Graphical Model Representations</li>
                    <li>Bishop Ch. 11: Sampling Methods</li>
                    <li>Cover & Thomas Ch. 2: Entropy, Relative Entropy</li>
                </ul>
            </div>
        </div>

        <!-- Navigation -->
        <div class="card mt-2">
            <div class="btn-group">
                <a href="syllabus.html" class="btn btn-secondary">← Syllabus</a>
                <a href="glossary.html" class="btn btn-primary">Key Terms →</a>
            </div>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <p>© 2026 SMU Lyle School of Engineering | Masters in Applied Artificial Intelligence</p>
        <p><a href="../../index.html">Program Home</a></p>
    </footer>
</body>

</html>